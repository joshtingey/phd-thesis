# CVN chapter plan

- Using cleverer algorithms has a smaller pay-off than you might expect, as to first order they all do the same thing, essentially grouping nearby examples into the same class.
- A dumb algorithm with lots of data tends to beat a clever one with modest amounts of it.
- You should try the simplest learners first. Decision tress before neural network before Convolutional neural networks.
- Combining different models in an ensemble greatly reduces variance while only slightly increasing bias (bagging, boosting, stacking).
- Few learners search their hypothesis space fully. Therefore, a learner with a large hypothesis space that tries fewer hypotheses from it is less likely to overfit than one that tries more hypotheses from a smaller space.
