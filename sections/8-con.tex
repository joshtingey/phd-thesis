\chapter{Summary and conclusion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{chap:conclusion} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This thesis presented a range of work for the \chips neutrino detector R\&D project. \chips puts
forward a novel water Cherenkov based concept to counter the vast expense, increased complexity,
and long construction time expected from future long-baseline neutrino oscillation experiments. As
detailed, this feat is possible via a series of steps: deploying detector modules in bodies of
water on the Earth's surface rather than deep underground; using commercially available rather
than bespoke components wherever possible; and optimising photocathode coverage to study
accelerator beam neutrinos exclusively. These steps reduce the total cost per kt of sensitive mass
to between \$200k-\$300k and should allow for megaton scale detectors to become a reality.

Moreover, \chips detector modules are expected to be relatively easy to build, quick to deploy,
and can be upgraded once operational, making them a much more attractive proposition when
resources are constrained. It is hoped that future \chips detectors will be able to help answer
some of the key unsolved questions of neutrino physics, such as the mass hierarchy ambiguity and
the search for CP violation in neutrino oscillations.

During the summer of 2019 a \chips prototype, \chipsfive was deployed into the Wentworth 2W
disused mine pit in northern Minnesota, USA. Although \chipsfive is not yet fully proven and
future plans are still in flux, the amount of knowledge gained during its construction,
deployment, and initial commissioning can not be overstated. Alongside proving that a large \chips
detector module can be constructed and deployed, \chipsfive acted as a brilliant testbed for the
development of the \chips data acquisition system. 

Notably, the use of commercially available single-board Beaglebone machines and the open-source
Elasticsearch monitoring solution were found to be highly successful. Within both future \chips
detectors, and hopefully the broader experimental particle physics field, bespoke components
should continue to be phased out, with commercially available or open-source components used
instead. Not only does this drastically reduce the implementation effort required by collaborators
and cost, but leads to a much improved final result due to the pooling of resources. 

As is the case within the world around us, future \chips concept detectors will also make
ever-increasing use of modern deep learning techniques. This comes as a direct result of the
dramatic improvement in \chipsfive reconstruction and classification performance brought about by
the principal work presented in this thesis. Three forms of a Convolutional Neural Network have
been trained to reject cosmic muon events, classify beam events, and estimate neutrino energies,
all using only the raw detector event as input. This new approach replaces the standard
likelihood-based reconstruction and simple neural network classification, greatly increasing
generalisability and processing speed.

With the primary goal of selecting an efficient and pure appeared CC $\nu_{e}$ sample for which
the neutrino energy can be accurately determined, the new CNN-based approach is found to provide
excellent performance. In some cases, the performance is comparable with similar experiments,
impressive given the significant differences in detector design. The vast cosmic muon background
of \chipsfive is found to be accepted by only a factor of $<9.5\times10^{-7}$ (without the help of
a veto), equivalent to $<2$ cosmic muon events contaminating the beam sample per year, of which
none are expected to be classified as CC $\nu_{e}$ events. 

Furthermore, the key performance metrics for both the CC $\nu_{e}$ and CC $\nu_{\mu}$ beam
selections are summarised in \TableRef{tab:final_metrics}, with a comparison of the metrics
available for the old likelihood-based approach given in \TableRef{tab:final_comparison}. The new
CNN approach is found to improve the primary CC $\nu_{e}$ selection signal efficiency by an
impressive $212\pm5\%$, and the corresponding signal purity by $180\pm6\%$. Energy estimation is
also significantly improved with the approximate energy resolution for CC $\nu_{e}$ QEL event
electrons $30\pm4\%$ that provided by the old likelihood-based method.

\begin{table}
    \begin{tabular}{lrrr}
        Selection           & Signal Efficiency & Signal Purity & Approximate $\nu$ Energy Resolution \\
        \midrule
        CC $\nu_{e}$     & $73.4\pm0.2\%$ & $70.9\pm0.6\%$ & $10.2\pm0.2\%$ \\
        CC $\nu_{\mu}$   & $37.0\pm0.1\%$ & $96.0\pm0.1\%$ & $12.5\pm0.2\%$ \\
    \end{tabular}
    \caption[Key performance metrics of the new CNN approach]
    {The key performance metrics for both the CC $\nu_{e}$ and CC $\nu_{\mu}$ beam selections
        using the new CNN-based approach. The signal efficiency relative to the total number of
        expected events, the signal purity defined as the fraction of selected events which are
        signal, and the approximate signal neutrino energy resolution. The values considering both
        the appeared and beam CC $\nu_{e}$ components as signal are given for the CC $\nu_{e}$
        selection.}
    \label{tab:final_metrics}
\end{table}

\begin{table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lrrr}
        Metric           & Likelihood-based (old) & CNN-based (new) & Percentage Change \\
        \midrule
        Max FOM-$\nu_{e}$            & $0.132\pm0.005$ & $0.519\pm0.004$ & $393\pm15\%$ \\
        CC $\nu_{e}$ Efficiency      & $34.7\pm0.8\%$  & $73.4\pm0.2\%$  & $212\pm5\%$  \\
        CC $\nu_{e}$ Purity          & $39.3\pm1.2\%$  & $70.9\pm0.6\%$  & $180\pm6\%$  \\
        \midrule
        QEL $\nu_{e}$ lepton energy res   & $14.1\pm1.7\%$  & $4.2\pm0.1\%$  & $30\pm4\%$ \\
        QEL $\nu_{\mu}$ lepton energy res & $9.9\pm1.6\%$  & $3.8\pm0.1\%$   & $39\pm6\%$ \\
    \end{tabular}
    } \caption[Performance comparison between the old likelihood-based approach and the new
    CNN-based approach] {Comparison of performance metrics between the old likelihood-based
    approach and the new CNN-based approach with the percentage change given for reference. The
    maximum FOM value metric alongside the signal efficiency and purity are given for the CC
    $\nu_{e}$ beam selection, as well as the QEL event approximate lepton energy resolutions for
    both CC $\nu_{e}$ and CC $\nu_{\mu}$ selected signal events.}
    \label{tab:final_comparison}
\end{table}

Not only are the trained CNNs found to provide excellent performance, but some insight into their
inner workings was achieved, and their outputs are found to be robust to a sample of tested
distributional changes in the input. These findings go some way to answering the common and
justified concern that they are too often used as a black box (input in, outputs out). Cherenkov
ring and Hough peak features are extracted from the input images, resulting in a learnt
representation of the inputs seen to have strong discriminating power between categories when
visualised using the t-SNE technique. Additionally, realistic modifications to the input hit times
and charges and the addition of random noise are all found to have a negligible effect on output
performance.

It is sincerely hoped that other water Cherenkov neutrino experiments will take inspiration from
and then build upon the work presented in this thesis for their own Convolutional Neural Network
implementations. Although the results presented in this work are incredibly compelling, there are
still clear avenues for exploration and improvement. These are all principally related to the
critical performance drivers outlined within this thesis. 

Firstly, generating the input event maps to focus on the underlying Cherenkov profiles is
incredibly important; therefore, any methodology to remove distortions further or more accurately
determine the interaction vertex position will be beneficial. Secondly, the distribution of events
(in energy or type) used within the training sample heavily impacts performance; thus, a
comprehensive study of this behaviour could optimise the sample used. Finally, multi-task learning
clearly shows promise, with further trial-and-error or a more generalised approach likely to
uncover additional valuable tasks. 

Likely, the true potential of these methods is just beginning to be realised. As is always the
case, only time will tell.