\chapter{Introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{chap:introduction} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{page}{17}  % Need to explicitly set page number due to hepthesis class!! CHECK THIS!!!

TODO: Write the Introduction

Consider a neutrino, an energetic one,

- For now this work aims to allow for optimisation of the \chips detector concept.
- drive the development help others
- for detector optimisation studies
- Main purpose for now is to allow for the exploration of different detector designs, for future
CHIPS detectors.

This thesis presents a broad range of work conducted for the CHerenkov detectors In mine PitS
(CHIPS) neutrino detector R\&D project. The main contribution comes from the application of
Convolutional Neural Networks, a type of "deep learning" machine learning algorithm, to the
reconstruction and classification of neutrino events within the CHIPS-5 prototype detector.

The neutrino physics history, theoretical background, and current status are detailed as
motivation for the CHIPS project. A full description of the CHIPS detector concept follows, with a
principle focus on the CHIPS-5 prototype detector module deployed within the NuMI neutrino beam in
the autumn of 2019. A particularly detailed overview of the data acquisition and monitoring
systems developed for the CHIPS-5 detector is also given.

The standard neutrino event reconstruction and classification methods to be replaced are discussed
alongside a review of the relevant neural network theory and other "deep learning" applications
within the field. The Convolutional Neural Network implementations for CHIPS are then outlined,
including that for cosmic muon rejection, beam event classification and energy estimation.

A comprehensive evaluation of the trained Convolutional Neural Networks is then presented.
Firstly, the final combined performance is determined and compared with similar experiments.
Secondly, the inner workings of the trained networks are explored. Thirdly, the robustness of the
network outputs to distributional input changes is studied. Finally, alternative implementations
are discussed to highlight the key factors driving performance.

